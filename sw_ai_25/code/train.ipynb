{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ded74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/bgem3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b495690",
   "metadata": {},
   "source": [
    "## Data Load & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1d9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train_file_name, test_file_name):\n",
    "        self.train_file_name = train_file_name\n",
    "        self.test_file_name = test_file_name\n",
    "        self.text_list, self.feature_list = None, None\n",
    "        self.train, self.test = None, None\n",
    "\n",
    "        print(f\"[Init] Dataset initialized with train: {train_file_name}, test: {test_file_name}\")\n",
    "\n",
    "    def get_cls_embedding_batch(self, texts, max_length=256, batch_size=32):\n",
    "        print(f\"[Embedding] Start embedding {len(texts)} texts (batch size: {batch_size})\")\n",
    "        MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "        model.eval()\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(texts), batch_size), desc=\"[Embedding batches]\"):\n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                inputs = tokenizer(\n",
    "                    batch_texts,\n",
    "                    return_tensors='pt',\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    padding='max_length'\n",
    "                )\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs)\n",
    "                cls_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.append(cls_emb)\n",
    "        embeddings = np.vstack(embeddings) \n",
    "        print(f\"[Embedding] Completed. Shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def load_train_csv(self):\n",
    "        print(f\"[Load] Loading train CSV: {self.train_file_name}\")\n",
    "        train = pd.read_csv(self.train_file_name)\n",
    "        print(f\"[Load] Loading test CSV: {self.test_file_name}\")\n",
    "        test = pd.read_csv(self.test_file_name)\n",
    "        print(f\"[Load] Loaded train shape: {train.shape}, test shape: {test.shape}\")\n",
    "        return train, test\n",
    "\n",
    "    def load_train_pickle(self):\n",
    "        print(f\"[Load] Loading train pickle: {self.train_file_name}\")\n",
    "        with open(self.train_file_name, \"rb\") as f:\n",
    "            train = pickle.load(f)\n",
    "        print(f\"[Load] Loading test pickle: {self.test_file_name}\")\n",
    "        with open(self.test_file_name, \"rb\") as f:\n",
    "            test = pickle.load(f)\n",
    "        self.train, self.test = train, test\n",
    "        print(f\"[Load] train_col_list: {list(train.columns)}\")\n",
    "        print(f\"[Load] test_col_list: {list(test.columns)}\")\n",
    "        return train, test\n",
    "\n",
    "    def set_list(self, emb_list, feature_list, text_list=None):\n",
    "        self.emb_list = emb_list\n",
    "        self.feature_list = feature_list\n",
    "        self.text_list = text_list\n",
    "        print(f\"[Set] emb_list: {emb_list}, feature_list: {feature_list}, text_list: {text_list}\")\n",
    "\n",
    "    def split_data(self, train):\n",
    "        print(f\"[Split] Splitting data with stratify on 'generated'\")\n",
    "        col_list = self.emb_list + self.feature_list + self.text_list\n",
    "        X = train[col_list]\n",
    "        y = train['generated']\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "        print(f\"[Split] X_train: {X_train.shape}, X_val: {X_val.shape}\")\n",
    "        return X_train, X_val, y_train, y_val\n",
    "    \n",
    "    def get_text_emb(self, text_list):\n",
    "        if text_list is None:\n",
    "            text_list = self.text_list\n",
    "        text_set = []\n",
    "        print(f\"[Text Embedding] Extracting embeddings for: {text_list}\")\n",
    "        for text in text_list:\n",
    "            print(f\"[Text Embedding] Embedding column: {text}\")\n",
    "            tmp = self.get_cls_embedding_batch(self.train[text].tolist())\n",
    "            text_set.append(tmp)\n",
    "        print(f\"[Text Embedding] All embeddings extracted. Shape: {[arr.shape for arr in text_set]}\")\n",
    "        return np.hstack(text_set)\n",
    "\n",
    "    def df_load_train_text_emb(self, X_train, X_val, emb_list):\n",
    "        print(f\"[DF Load] Loading train/val text embeddings for: {emb_list}\")\n",
    "        train_matrix_set = []\n",
    "        val_matrix_set = []\n",
    "        for col in emb_list:\n",
    "            print(f\"[DF Load] Processing train column: {col}\")\n",
    "            tmp = np.vstack(X_train[col].tolist())\n",
    "            train_matrix_set.append(tmp)\n",
    "        for col in emb_list:\n",
    "            print(f\"[DF Load] Processing val column: {col}\")\n",
    "            tmp = np.vstack(X_val[col].tolist())\n",
    "            val_matrix_set.append(tmp)\n",
    "        train_text_matrix = np.hstack(train_matrix_set)\n",
    "        val_text_matrix = np.hstack(val_matrix_set)\n",
    "        print(f\"[DF Load] train_text_matrix shape: {train_text_matrix.shape}, val_text_matrix shape: {val_text_matrix.shape}\")\n",
    "        return train_text_matrix, val_text_matrix\n",
    "    \n",
    "    def df_load_test_text_emb(self, X_test, emb_list):\n",
    "        if emb_list is None:\n",
    "            emb_list = self.emb_list\n",
    "        print(f\"[DF Load Test] Loading test text embeddings for: {emb_list}\")\n",
    "        test_matrix_set = []\n",
    "        for col in emb_list:\n",
    "            print(f\"[DF Load Test] Processing test column: {col}\")\n",
    "            tmp = np.vstack(X_test[col].tolist())\n",
    "            test_matrix_set.append(tmp)\n",
    "        test_text_matrix = np.hstack(test_matrix_set)\n",
    "        print(f\"[DF Load Test] test_text_matrix shape: {test_text_matrix.shape}\")\n",
    "        return test_text_matrix\n",
    "    \n",
    "    def concat_train_feature(self, X_train, X_val, train_text_matrix, \n",
    "                                                    val_text_matrix, \n",
    "                                                    feature_list):\n",
    "        print(f\"[Concat] Concatenating features: {feature_list}\")\n",
    "        train_feature_matrix = X_train[feature_list].to_numpy()\n",
    "        train_full_matrix = np.hstack([train_text_matrix, train_feature_matrix])\n",
    "        val_feature_matrix = X_val[feature_list].to_numpy()\n",
    "        val_full_matrix = np.hstack([val_text_matrix, val_feature_matrix])\n",
    "        print(f\"[Concat] train_full_matrix shape: {train_full_matrix.shape}, val_full_matrix shape: {val_full_matrix.shape}\")\n",
    "        return train_full_matrix, val_full_matrix\n",
    "\n",
    "    def concat_test_feature(self, test_text_matrix, feature_list):\n",
    "        print(f\"[Concat Test] Concatenating test features: {feature_list}\")\n",
    "        test_feature_matrix = self.test[feature_list].to_numpy()\n",
    "        test_full_matrix = np.hstack([test_text_matrix, test_feature_matrix])\n",
    "        print(f\"[Concat Test] test_full_matrix shape: {test_full_matrix.shape}\")\n",
    "        return test_full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88287211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Dataset initialized with train: ../data/train_emb_change.pkl, test: ../data/test_emb_change.pkl\n",
      "[Load] Loading train pickle: ../data/train_emb_change.pkl\n",
      "[Load] Loading test pickle: ../data/test_emb_change.pkl\n",
      "[Load] train_col_list: ['paragraph_index', 'generated', 'paragraph_text_emb', 'full_text_emb', 'adj_emb_change', 'title_emb']\n",
      "[Load] test_col_list: ['ID', 'title', 'paragraph_index', 'paragraph_text', 'paragraph_text_emb', 'title_emb', 'adj_emb_change']\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(\"../data/train_emb_change.pkl\", \"../data/test_emb_change.pkl\")\n",
    "train, test = dataset.load_train_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7565cbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Set] emb_list: ['full_text_emb', 'title_emb'], feature_list: ['adj_emb_change'], text_list: []\n"
     ]
    }
   ],
   "source": [
    "text_list=[]\n",
    "feature_list=[\"adj_emb_change\"]\n",
    "emb_list=[\"full_text_emb\",\"title_emb\"]\n",
    "dataset.set_list(emb_list, feature_list, text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbf3043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Split] Splitting data with stratify on 'generated'\n",
      "[Split] X_train: (981091, 3), X_val: (245273, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = dataset.split_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4721dec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DF Load] Loading train/val text embeddings for: ['full_text_emb', 'title_emb']\n",
      "[DF Load] Processing train column: full_text_emb\n",
      "[DF Load] Processing train column: title_emb\n",
      "[DF Load] Processing val column: full_text_emb\n",
      "[DF Load] Processing val column: title_emb\n",
      "[DF Load] train_text_matrix shape: (981091, 1536), val_text_matrix shape: (245273, 1536)\n"
     ]
    }
   ],
   "source": [
    "train_text_matrix, val_text_matrix = dataset.df_load_train_text_emb(X_train, X_val, emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b96f1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Concat] Concatenating features: ['adj_emb_change']\n",
      "[Concat] train_full_matrix shape: (981091, 1537), val_full_matrix shape: (245273, 1537)\n"
     ]
    }
   ],
   "source": [
    "train_full_matrix, val_full_matrix= dataset.concat_train_feature(X_train, X_val,\n",
    "                                                                 train_text_matrix, \n",
    "                                                                 val_text_matrix, \n",
    "                                                                 feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171fab8d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd25d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.rename(columns={\"paragraph_text_emb\":\"full_text_emb\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7d07dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DF Load Test] Loading test text embeddings for: ['full_text_emb', 'title_emb']\n",
      "[DF Load Test] Processing test column: full_text_emb\n",
      "[DF Load Test] Processing test column: title_emb\n",
      "[DF Load Test] test_text_matrix shape: (1962, 1536)\n",
      "[Concat Test] Concatenating test features: ['adj_emb_change']\n",
      "[Concat Test] test_full_matrix shape: (1962, 1537)\n"
     ]
    }
   ],
   "source": [
    "test_text_matrix = dataset.df_load_test_text_emb(test, emb_list)\n",
    "test_full_matrix = dataset.concat_test_feature(test_text_matrix, feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=42)\n",
    "xgb.fit(train_full_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "420c29a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.9918\n"
     ]
    }
   ],
   "source": [
    "val_probs = xgb.predict_proba(val_full_matrix)[:, 1]\n",
    "auc = roc_auc_score(y_val, val_probs)\n",
    "print(f\"Validation AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a37aa10",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e48dd2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = xgb.predict_proba(test_full_matrix)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950e22ec",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47d0c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('../data/sample_submission.csv', encoding='utf-8-sig')\n",
    "sample_submission['generated'] = probs\n",
    "\n",
    "sample_submission.to_csv(f'../output/baseline_submission_change.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8becd47e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgem3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
