{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ded74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b495690",
   "metadata": {},
   "source": [
    "## Data Load & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train_file_name, test_file_name):\n",
    "        self.train_file_name = train_file_name\n",
    "        self.test_file_name = test_file_name\n",
    "        self.train_emb_list, self.test_emb_list = None, None\n",
    "        self.text_list, self.feature_list = None, None\n",
    "        self.train, self.test = None, None\n",
    "\n",
    "        print(f\"[Init] Dataset initialized with train: {train_file_name}, test: {test_file_name}\")\n",
    "\n",
    "    def get_cls_embedding_batch(self, texts, max_length=256, batch_size=32):\n",
    "        print(f\"[Embedding] Start embedding {len(texts)} texts (batch size: {batch_size})\")\n",
    "        MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "        model.eval()\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(texts), batch_size), desc=\"[Embedding batches]\"):\n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                inputs = tokenizer(\n",
    "                    batch_texts,\n",
    "                    return_tensors='pt',\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    padding='max_length'\n",
    "                )\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs)\n",
    "                cls_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.append(cls_emb)\n",
    "        embeddings = np.vstack(embeddings) \n",
    "        print(f\"[Embedding] Completed. Shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def load_train_csv(self):\n",
    "        print(f\"[Load] Loading train CSV: {self.train_file_name}\")\n",
    "        train = pd.read_csv(self.train_file_name)\n",
    "        print(f\"[Load] Loading test CSV: {self.test_file_name}\")\n",
    "        test = pd.read_csv(self.test_file_name)\n",
    "        print(f\"[Load] Loaded train shape: {train.shape}, test shape: {test.shape}\")\n",
    "        return train, test\n",
    "\n",
    "    def load_train_pickle(self):\n",
    "        print(f\"[Load] Loading train pickle: {self.train_file_name}\")\n",
    "        with open(self.train_file_name, \"rb\") as f:\n",
    "            train = pickle.load(f)\n",
    "        print(f\"[Load] Loading test pickle: {self.test_file_name}\")\n",
    "        with open(self.test_file_name, \"rb\") as f:\n",
    "            test = pickle.load(f)\n",
    "        self.train, self.test = train, test\n",
    "        print(f\"[Load] train_col_list: {list(train.columns)}\")\n",
    "        print(f\"[Load] test_col_list: {list(test.columns)}\")\n",
    "        return train, test\n",
    "\n",
    "    def set_list(self, train_emb_list, test_emb_list, feature_list, text_list=None):\n",
    "        self.train_emb_list = train_emb_list\n",
    "        self.test_emb_list = test_emb_list\n",
    "        self.feature_list = feature_list\n",
    "        self.text_list = text_list\n",
    "        print(f\"[Set] train_emb_list: {train_emb_list}, \\n test_emb_list: {test_emb_list}, \\nfeature_list: {feature_list}, \\ntext_list: {text_list}\")\n",
    "\n",
    "    def split_data(self, train):\n",
    "        print(f\"[Split] Splitting data with stratify on 'generated'\")\n",
    "        col_list = self.train_emb_list + self.feature_list + self.text_list\n",
    "        X = train[col_list]\n",
    "        y = train['generated']\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "        print(f\"[Split] X_train: {X_train.shape}, X_val: {X_val.shape}\")\n",
    "        return X_train, X_val, y_train, y_val\n",
    "    \n",
    "    def get_text_emb(self, text_list):\n",
    "        if text_list is None:\n",
    "            text_list = self.text_list\n",
    "        text_set = []\n",
    "        print(f\"[Text Embedding] Extracting embeddings for: {text_list}\")\n",
    "        for text in text_list:\n",
    "            print(f\"[Text Embedding] Embedding column: {text}\")\n",
    "            tmp = self.get_cls_embedding_batch(self.train[text].tolist())\n",
    "            text_set.append(tmp)\n",
    "        print(f\"[Text Embedding] All embeddings extracted. Shape: {[arr.shape for arr in text_set]}\")\n",
    "        return np.hstack(text_set)\n",
    "\n",
    "    def df_load_train_text_emb(self, X_train, X_val, train_emb_list):\n",
    "        if train_emb_list is None:\n",
    "            train_emb_list = self.train_emb_list\n",
    "        print(f\"[DF Load] Loading train/val text embeddings for: {train_emb_list}\")\n",
    "        train_matrix_set = []\n",
    "        val_matrix_set = []\n",
    "        for col in train_emb_list:\n",
    "            print(f\"[DF Load] Processing train column: {col}\")\n",
    "            tmp = np.vstack(X_train[col].tolist())\n",
    "            train_matrix_set.append(tmp)\n",
    "        for col in train_emb_list:\n",
    "            print(f\"[DF Load] Processing val column: {col}\")\n",
    "            tmp = np.vstack(X_val[col].tolist())\n",
    "            val_matrix_set.append(tmp)\n",
    "        train_text_matrix = np.hstack(train_matrix_set)\n",
    "        val_text_matrix = np.hstack(val_matrix_set)\n",
    "        print(f\"[DF Load] train_text_matrix shape: {train_text_matrix.shape}, val_text_matrix shape: {val_text_matrix.shape}\")\n",
    "        return train_text_matrix, val_text_matrix\n",
    "    \n",
    "    def df_load_test_text_emb(self, X_test, test_emb_list):\n",
    "        if test_emb_list is None:\n",
    "            test_emb_list = self.test_emb_list\n",
    "        print(f\"[DF Load Test] Loading test text embeddings for: {test_emb_list}\")\n",
    "        test_matrix_set = []\n",
    "        for col in test_emb_list:\n",
    "            print(f\"[DF Load Test] Processing test column: {col}\")\n",
    "            tmp = np.vstack(X_test[col].tolist())\n",
    "            test_matrix_set.append(tmp)\n",
    "        test_text_matrix = np.hstack(test_matrix_set)\n",
    "        print(f\"[DF Load Test] test_text_matrix shape: {test_text_matrix.shape}\")\n",
    "        return test_text_matrix\n",
    "    \n",
    "    def scaled_matrix(self, matrix):\n",
    "        scaler = StandardScaler()\n",
    "        matrix_scaled = scaler.fit_transform(matrix)\n",
    "        return matrix_scaled\n",
    "\n",
    "    def concat_train_feature(self, X_train, X_val, train_text_matrix, \n",
    "                                                    val_text_matrix, \n",
    "                                                    feature_list):\n",
    "        print(f\"[Concat] Concatenating features: {feature_list}\")\n",
    "        train_feature_matrix = X_train[feature_list].to_numpy()\n",
    "        val_feature_matrix = X_val[feature_list].to_numpy()\n",
    "\n",
    "        # train_feature_matrix_scaled = self.scaled_matrix(train_feature_matrix)\n",
    "        # val_feature_matrix_scaled = self.val_matrix(train_feature_matrix)\n",
    "\n",
    "        train_full_matrix = np.hstack([train_text_matrix, train_feature_matrix])\n",
    "        val_full_matrix = np.hstack([val_text_matrix, val_feature_matrix])\n",
    "        print(f\"[Concat] train_full_matrix shape: {train_full_matrix.shape}, val_full_matrix shape: {val_full_matrix.shape}\")\n",
    "        return train_full_matrix, val_full_matrix\n",
    "\n",
    "    def concat_test_feature(self, test_text_matrix, feature_list):\n",
    "        print(f\"[Concat Test] Concatenating test features: {feature_list}\")\n",
    "        test_feature_matrix = self.test[feature_list].to_numpy()\n",
    "        # test_feature_matrix_scaled = self.scaled_matrix(test_feature_matrix)\n",
    "        test_full_matrix = np.hstack([test_text_matrix, test_feature_matrix])\n",
    "        print(f\"[Concat Test] test_full_matrix shape: {test_full_matrix.shape}\")\n",
    "        return test_full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "    def __init__(self, train_matrix, val_matrix, test_matrix):\n",
    "        self.train_matrix = train_matrix\n",
    "        self.val_matrix = val_matrix\n",
    "        self.test_matrix = test_matrix\n",
    "        self.feature_df = None\n",
    "    \n",
    "    def scaled_matrix(self, matrix):\n",
    "        scaler = StandardScaler()\n",
    "        matrix_scaled = scaler.fit_transform(matrix)\n",
    "        return matrix_scaled\n",
    "\n",
    "    # TODO\n",
    "    \"\"\"\n",
    "    def feature enginnering() -> Dataframe:\n",
    "        특징 추출..\n",
    "        \n",
    "    def make_feature_df() -> Dataframe:\n",
    "        ..\n",
    "        return feature_df \n",
    "\n",
    "    etc..\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def concat_feature(self, matrix, feature_df):\n",
    "        if feature_df is None:\n",
    "            feature_df = self.feature_df\n",
    "        col_list=list(feature_df.columns)\n",
    "        feature_matrix=feature_df[col_list].to_numpy()\n",
    "        # feature_matrix_scared = self.scaled_matrix(feature_matrix)\n",
    "        return np.hstack([matrix, feature_matrix])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88287211",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\"../data/train_paraemb_change_ngram.pkl\", \"../data/test_paraemb_change_ngram.pkl\")\n",
    "train, test = dataset.load_train_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7565cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=[]\n",
    "feature_list=[\"paragraph_index\",\"adj_emb_change\",\"ngram_total\",\"ngram_unique\",\"ngram_max_freq\",\"ngram_diversity\",\"similarity\"]\n",
    "train_emb_list=[\"paragraph_text_emb\",\"title_emb\"]\n",
    "test_emb_list=[\"paragraph_text_emb\", \"title_emb\"]\n",
    "dataset.set_list(train_emb_list, test_emb_list, feature_list, text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = dataset.split_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_matrix, val_text_matrix = dataset.df_load_train_text_emb(X_train, X_val, train_emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_matrix, val_full_matrix= dataset.concat_train_feature(X_train, X_val,\n",
    "                                                                 train_text_matrix, \n",
    "                                                                 val_text_matrix, \n",
    "                                                                 feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171fab8d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd25d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.rename(columns={\"paragraph_text_emb\":\"paragraph_text_emb\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_matrix = dataset.df_load_test_text_emb(test, test_emb_list)\n",
    "test_full_matrix = dataset.concat_test_feature(test_text_matrix, feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_pos=np.sum(train[\"generated\"]==1)\n",
    "# n_neg=np.sum(train[\"generated\"]==0)\n",
    "# scale_pos_weight = n_neg / n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=42)\n",
    "xgb.fit(train_full_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_probs = xgb.predict_proba(val_full_matrix)[:, 1]\n",
    "auc = roc_auc_score(y_val, val_probs)\n",
    "print(f\"Validation AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a37aa10",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48dd2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = xgb.predict_proba(test_full_matrix)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950e22ec",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../data/sample_submission.csv', encoding='utf-8-sig')\n",
    "sample_submission['generated'] = probs\n",
    "\n",
    "sample_submission.to_csv(f'../output/baseline_submission_para_change_ngram_sim_scaled.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgem3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
